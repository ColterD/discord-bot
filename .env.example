# =============================================================================
# Discord Bot Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# All env vars are optional with sensible defaults unless marked [REQUIRED]
# =============================================================================

# -----------------------------------------------------------------------------
# Discord Configuration [REQUIRED]
# -----------------------------------------------------------------------------
# Bot display name (used in logs, embeds, and system messages)
BOT_NAME=Discord Bot

# Bot token from Discord Developer Portal
DISCORD_TOKEN=your_discord_bot_token_here

# Application Client ID from Discord Developer Portal
DISCORD_CLIENT_ID=your_client_id_here

# Guild ID for development (guild commands update instantly)
# Leave empty for production (global commands take ~1hr to propagate)
DEV_GUILD_ID=your_dev_guild_id_here

# -----------------------------------------------------------------------------
# Environment
# -----------------------------------------------------------------------------
# 'development' | 'production'
NODE_ENV=development

# Logging level: DEBUG | INFO | WARN | ERROR
LOG_LEVEL=DEBUG

# -----------------------------------------------------------------------------
# LLM Configuration (Ollama)
# -----------------------------------------------------------------------------
# Ollama API endpoint
# Docker: http://ollama:11434 | Local: http://localhost:11434
OLLAMA_HOST=http://ollama:11434

# Primary model for chat (supports GGUF from HuggingFace)
LLM_MODEL=hf.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf:Q5_1

# Fallback model when VRAM is constrained
LLM_FALLBACK_MODEL=qwen2.5:7b

# Max tokens for responses
LLM_MAX_TOKENS=4096

# Temperature (0.0-2.0, lower = more focused, higher = more creative)
LLM_TEMPERATURE=0.7

# Request timeout in milliseconds (default: 5 minutes)
LLM_REQUEST_TIMEOUT=300000

# Keep model in GPU memory (seconds, -1 = forever)
LLM_KEEP_ALIVE=300

# Preload model on startup (true/false)
LLM_PRELOAD=true

# Sleep after inactivity (milliseconds, default: 5 minutes)
LLM_SLEEP_AFTER_MS=300000

# Use orchestrator for tool-aware conversations (true/false)
LLM_USE_ORCHESTRATOR=true

# HERETIC model specific settings (MoE optimization)
LLM_NUM_EXPERTS=5
LLM_REP_PEN=1.1
LLM_TEMP_CODING=0.6
LLM_TEMP_CREATIVE=1.0
LLM_CONTEXT_LENGTH=8192

# Summarization model (runs on CPU to preserve GPU VRAM)
SUMMARIZATION_MODEL=qwen2.5:3b

# Embedding model for vector memory (runs on CPU)
EMBEDDING_MODEL=qwen3-embedding:0.6b

# -----------------------------------------------------------------------------
# Valkey (Redis-compatible cache for active context)
# -----------------------------------------------------------------------------
VALKEY_URL=valkey://valkey:6379

# Conversation TTL in milliseconds (default: 30 minutes)
VALKEY_CONVERSATION_TTL_MS=1800000

# Key prefix for namespacing
VALKEY_KEY_PREFIX=discord-bot:

# -----------------------------------------------------------------------------
# ChromaDB (Vector Store for long-term memory)
# -----------------------------------------------------------------------------
CHROMA_URL=http://chromadb:8000
CHROMA_COLLECTION=memories

# -----------------------------------------------------------------------------
# Memory Configuration (Three-tier architecture)
# -----------------------------------------------------------------------------
# Master switch for memory system
MEMORY_ENABLED=true

# Summarization triggers
MEMORY_SUMMARIZE_AFTER_MESSAGES=15
MEMORY_SUMMARIZE_AFTER_IDLE_MS=1800000

# Context window allocation (tokens)
MEMORY_MAX_CONTEXT_TOKENS=4096

# Relevance thresholds for memory retrieval (0.0-1.0)
MEMORY_PROFILE_THRESHOLD=0.4
MEMORY_EPISODIC_THRESHOLD=0.55

# Time decay - older memories weighted less (multiplier per day, 0.5-1.0)
MEMORY_TIME_DECAY_PER_DAY=0.98

# Minimum importance score for memories to be stored (0.0-1.0)
MEMORY_MIN_IMPORTANCE=0.3

# -----------------------------------------------------------------------------
# SearXNG (Web Search)
# -----------------------------------------------------------------------------
SEARXNG_URL=http://searxng:8080
SEARXNG_TIMEOUT=30000

# -----------------------------------------------------------------------------
# MCP (Model Context Protocol)
# -----------------------------------------------------------------------------
# Config file location for stdio-based MCP servers
MCP_CONFIG_PATH=./mcp-servers.json

# Timeouts in milliseconds
MCP_CONNECTION_TIMEOUT_MS=30000
MCP_REQUEST_TIMEOUT_MS=60000

# Docker MCP Gateway Configuration (Docker Desktop MCP Toolkit)
# Master switch to enable Docker MCP Gateway
DOCKER_MCP_ENABLED=false

# Transport type: "stdio" (recommended, spawns gateway process) or "http" (StreamableHTTP)
DOCKER_MCP_TRANSPORT=stdio

# HTTP Transport Settings (only used when DOCKER_MCP_TRANSPORT=http)
DOCKER_MCP_GATEWAY_URL=http://host.docker.internal:8811
DOCKER_MCP_GATEWAY_ENDPOINT=/mcp
DOCKER_MCP_BEARER_TOKEN=

# Reconnection settings
DOCKER_MCP_AUTO_RECONNECT=true
DOCKER_MCP_MAX_RECONNECT_ATTEMPTS=5

# -----------------------------------------------------------------------------
# Security / Permissions
# -----------------------------------------------------------------------------
# Bot permission system (comma-separated Discord user IDs)
BOT_OWNER_IDS=your_discord_user_id_here
BOT_ADMIN_IDS=
BOT_MODERATOR_IDS=

# Impersonation detection
SECURITY_IMPERSONATION_ENABLED=true
SECURITY_SIMILARITY_THRESHOLD=0.7

# -----------------------------------------------------------------------------
# ComfyUI (Image Generation)
# -----------------------------------------------------------------------------
COMFYUI_URL=http://comfyui:8188
COMFYUI_MAX_QUEUE=5
COMFYUI_TIMEOUT=120000
COMFYUI_SLEEP_AFTER_MS=300000
COMFYUI_UNLOAD_ON_SLEEP=true

# -----------------------------------------------------------------------------
# GPU / VRAM Configuration (for RTX 4090 with 24GB VRAM)
# -----------------------------------------------------------------------------
# Total VRAM available in MB (RTX 4090=24576, RTX 4080=16384, RTX 3090=24576)
GPU_TOTAL_VRAM_MB=24576

# Minimum free VRAM buffer to maintain (MB)
GPU_MIN_FREE_MB=2048

# VRAM usage thresholds (0.0-1.0)
GPU_WARNING_THRESHOLD=0.75
GPU_CRITICAL_THRESHOLD=0.90

# Estimated VRAM usage per task (MB)
GPU_LLM_VRAM_MB=14000
GPU_IMAGE_VRAM_MB=8000

# VRAM monitoring interval (ms)
GPU_POLL_INTERVAL_MS=5000

# Auto-unload LLM for image generation if VRAM is tight
GPU_AUTO_UNLOAD_FOR_IMAGES=true

# Inactivity timeout before unloading models from VRAM (milliseconds, default: 5 minutes)
COMFYUI_SLEEP_AFTER_MS=300000

# Whether to unload models when sleeping to free VRAM (true/false, default: true)
COMFYUI_UNLOAD_ON_SLEEP=true

# -----------------------------------------------------------------------------
# Rate Limiting
# -----------------------------------------------------------------------------
RATE_LIMIT_REQUESTS=10
RATE_LIMIT_WINDOW_MS=60000

# -----------------------------------------------------------------------------
# Testing Configuration
# -----------------------------------------------------------------------------
# Master switch for test mode (enables test channels and verbose logging)
TEST_MODE=false

# Webhook URL for automated testing
TEST_WEBHOOK_URL=

# Channels where bot responds to ALL messages (comma-separated channel IDs)
TEST_CHANNEL_IDS=

# Enable verbose logging for test channels
TEST_VERBOSE_LOGGING=false

# Fake token pattern for unit tests (should be obviously fake but valid format)
TEST_DISCORD_TOKEN_PATTERN=MTIzNDU2Nzg5MDEyMzQ1Njc4OTAx.G12345._test_token_not_real_do_not_use_

# -----------------------------------------------------------------------------
# Troubleshooting Notes
# -----------------------------------------------------------------------------
# If you see "Failed to connect to OAuth notifications" errors:
#   docker mcp feature disable mcp-oauth-dcr
# This is a known issue (GitHub: docker/mcp-gateway#245)
